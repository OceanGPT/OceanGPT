<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>
<br>
<div align="center">
<img src="figs/logo.jpg" width="300px">

**OceanGPT (æ²§æ¸Š): ä¸€ä¸ªé¢å‘æµ·æ´‹ç§‘å­¦ä»»åŠ¡çš„å¤§è¯­è¨€æ¨¡å‹**

<p align="center">
    <a href="https://github.com/zjunlp/OceanGPT">é¡¹ç›®</a> â€¢
    <a href="https://arxiv.org/abs/2310.02031">è®ºæ–‡</a> â€¢
    <a href="https://huggingface.co/collections/zjunlp/oceangpt-664cc106358fdd9f09aa5157">æ¨¡å‹</a> â€¢
    <a href="http://oceangpt.zjukg.cn/">ç½‘ç«™</a> â€¢
    <a href="https://ajar-mayflower-ac1.notion.site/OceanGPT-1f8204ef4eed80db8842c3925dc9b814">æ‰‹å†Œ</a> â€¢
    <a href="#æ¦‚è¿°">æ¦‚è¿°</a> â€¢
    <a href="#å¿«é€Ÿå¼€å§‹">å¿«é€Ÿå¼€å§‹</a> â€¢
    <a href="#å¼•ç”¨">å¼•ç”¨</a>
</p>

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
![](https://img.shields.io/badge/PRs-Welcome-red)  <a href='https://hyper.ai/datasets/32992'><img src='https://img.shields.io/badge/Dataset-HyperAIè¶…ç¥ç»-pink'></a> 


</div>


**âœ¨ [OceanGPT æ–°æ‰‹æ•™ç¨‹](https://ajar-mayflower-ac1.notion.site/OceanGPT-1f8204ef4eed80db8842c3925dc9b814)|[æ–°æ‰‹æ•™ç¨‹ä¸­æ–‡ç‰ˆ](https://www.notion.so/OceanGPT-V1-0-225204ef4eed802584d2f77d6d2d5f3e)æ­£å¼å‘å¸ƒï¼**

**âœ¨ [OceanGPT å¾®è°ƒæ•™ç¨‹](https://www.notion.so/Fine-Tuning-OceanGPT-for-Task-Oriented-QA-243204ef4eed80bfb47de1acdad24e96)|[å®šåˆ¶é—®ç­”å¼•æ“æ•™ç¨‹ä¸­æ–‡ç‰ˆ](https://www.notion.so/OceanGPT-242204ef4eed809d8ef5e452bf294da7)æ­£å¼å‘å¸ƒï¼**

æˆ‘ä»¬å‘å¸ƒäº†è¯¦ç»†çš„OceanGPTæ–°æ‰‹æ•™ç¨‹ï¼Œå¸®åŠ©æ‚¨å¿«é€Ÿäº†è§£å…¶åŠŸèƒ½ã€‚å¦‚æœæ‚¨æƒ³è¦ä¸ºå®é™…ä½¿ç”¨å®šåˆ¶OceanGPTï¼Œå¯ä»¥å‚è€ƒå¾®è°ƒæ•™ç¨‹æ¥æ„å»ºå®šåˆ¶çš„é—®ç­”å¼•æ“ã€‚



> [!IMPORTANT]
> æˆ‘ä»¬å®šæœŸ**æ›´æ–°æˆ‘ä»¬çš„å¼€æºæ¨¡å‹**ï¼Œå› æ­¤å®ƒä»¬çš„åŠŸèƒ½å¯èƒ½ä¸ä¹‹å‰çš„ç‰ˆæœ¬æœ‰æ‰€ä¸åŒã€‚æˆ‘ä»¬çƒ­çƒˆæ¬¢è¿æ‚¨çš„åé¦ˆï¼Œä»¥å¸®åŠ©æˆ‘ä»¬æŒç»­æ”¹è¿›LLMåœ¨æµ·æ´‹é¢†åŸŸçš„åº”ç”¨ã€‚

## ç›®å½•

- <a href="#æœ€æ–°åŠ¨æ€">æœ€æ–°åŠ¨æ€</a>
- <a href="#æ¦‚è¿°">æ¦‚è¿°</a>
- <a href="#å¿«é€Ÿå¼€å§‹">å¿«é€Ÿå¼€å§‹</a>
- <a href="#ä¸æˆ‘ä»¬çš„Gradioæ¼”ç¤ºå¯¹è¯"> ğŸ¤—ä¸æˆ‘ä»¬çš„Gradioæ¼”ç¤ºå¯¹è¯</a>
- <a href="#åŸºäº OceanGPT æ„å»ºå®šåˆ¶åŒ–é—®ç­”åº”ç”¨">åŸºäº OceanGPT æ„å»ºå®šåˆ¶åŒ–é—®ç­”åº”ç”¨</a>
- <a href="#ğŸ“Œæ¨ç†">æ¨ç†</a>
    - <a href="#æ¨¡å‹">æ¨¡å‹</a>
    - <a href="#ä½¿ç”¨sglangã€vLLMã€ollamaã€llama.cppè¿›è¡Œé«˜æ•ˆæ¨ç†">ä½¿ç”¨sglangã€vLLMã€ollamaã€llama.cppè¿›è¡Œé«˜æ•ˆæ¨ç†</a>
- <a href="#è‡´è°¢">è‡´è°¢</a>
- <a href="#å±€é™æ€§">å±€é™æ€§</a>
- <a href="#å¼•ç”¨">å¼•ç”¨</a>

## ğŸ””æœ€æ–°åŠ¨æ€
- **2025-08-05ï¼Œæˆ‘ä»¬å‘å¸ƒäº†å…³äºå¾®è°ƒOceanGPTæ¨¡å‹è¿›è¡Œä»»åŠ¡å¯¼å‘é—®ç­”ä»»åŠ¡çš„[æ•™ç¨‹](https://github.com/zjunlp/OceanGPT/blob/main/CustomQA_EN.md)ã€‚**
- **2025-06-17ï¼Œæˆ‘ä»¬å‘å¸ƒäº†OceanGPT-coder-0.6Bã€‚**
- **2025-05-29ï¼Œæˆ‘ä»¬éƒ¨ç½²äº†OceanGPT MCPæœåŠ¡å™¨ä»¥æ”¯æŒå£°çº³å›¾åƒè§£é‡Šã€‚**
- **2025-04-20ï¼Œæˆ‘ä»¬å‘å¸ƒäº†OceanGPT-o-7Bå’ŒOceanGPT-coder-7Bã€‚**
- **2025-02-01ï¼Œæˆ‘ä»¬æ”¶é›†å£°çº³æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒå¹¶æµ‹è¯•OceanGPT-coderã€‚**
- **2024-12-01ï¼Œæˆ‘ä»¬æ”¶é›†æ›´å¤šå…¬å¼€å¯ç”¨çš„å£°çº³æ•°æ®å’Œç§‘å­¦å›¾åƒè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚**
- **2024-08-01ï¼Œæˆ‘ä»¬å¯åŠ¨äº†åŒè¯­ï¼ˆä¸­è‹±æ–‡ï¼‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹OceanGPT-oï¼Œæ”¶é›†å’Œè®­ç»ƒå£°çº³å’Œæµ·æ´‹ç§‘å­¦å›¾åƒæ•°æ®ã€‚**
- **2024-07-04ï¼Œæˆ‘ä»¬å‘å¸ƒäº†OceanGPT-basic-14B/2Bå’Œæ›´æ–°ç‰ˆæœ¬çš„OceanGPT-basic-7Bï¼ˆv0.2ï¼‰ã€‚**
- **2024-06-04ï¼Œ[OceanGPT](https://arxiv.org/abs/2310.02031)è¢«ACL 2024æ¥æ”¶ã€‚ğŸ‰ğŸ‰**
- **2023-10-04ï¼Œæˆ‘ä»¬å‘å¸ƒäº†è®ºæ–‡"[OceanGPT: A Large Language Model for Ocean Science Tasks](https://arxiv.org/abs/2310.02031)"å¹¶å‘å¸ƒäº†åŸºäºLLaMA2çš„OceanGPT-basic-7Bï¼ˆv0.1ï¼‰ã€‚**
- **2023-05-01ï¼Œæˆ‘ä»¬å¯åŠ¨äº†OceanGPTï¼ˆæ²§æ¸Šï¼‰é¡¹ç›®ã€‚**
---
### æ¨¡å‹

| æ¨¡å‹åç§°        |        ModelScope                                                                                                              | HuggingFace                                                               |
|-------------------|-----------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|
| OceanGPT-o-7B (åŸºäºQwenï¼Œ**æ¨è**)                      |<a href="https://modelscope.cn/models/ZJUNLP/OceanGPT-o-7B" target="_blank">7B</a>   | <a href="https://huggingface.co/zjunlp/OceanGPT-o-7B" target="_blank">7B</a> |
| OceanGPT-coder-7B (åŸºäºQwenï¼Œ**æ¨è**)                                                                      | <a href="https://modelscope.cn/models/ZJUNLP/OceanGPT-coder-7B" target="_blank">7B</a>                                                                        | <a href="https://huggingface.co/zjunlp/OceanGPT-coder-7B" target="_blank">7B</a>
| OceanGPT-basic-8B (åŸºäºQwenï¼Œ**æ¨è**) |<a href="https://www.modelscope.cn/models/ZJUNLP/OceanGPT-basic-8B" target="_blank">8B</a>   | <a href="https://huggingface.co/zjunlp/OceanGPT-basic-8B" target="_blank">8B</a> |
| OceanGPT-basic-14B (åŸºäºQwenï¼Œæ—§ç‰ˆ) |<a href="https://modelscope.cn/models/ZJUNLP/OceanGPT-14B-v0.1" target="_blank">14B</a>   | <a href="https://huggingface.co/zjunlp/OceanGPT-14B-v0.1" target="_blank">14B</a> |
| OceanGPT-basic-7B (åŸºäºQwenï¼Œæ—§ç‰ˆ) |  <a href="https://modelscope.cn/models/ZJUNLP/OceanGPT-7b-v0.2" target="_blank">7B</a>    |  <a href="https://huggingface.co/zjunlp/OceanGPT-7b-v0.2" target="_blank">7B</a>   |
| OceanGPT-basic-2B (åŸºäºMiniCPMï¼Œæ—§ç‰ˆ) | <a href="https://modelscope.cn/models/ZJUNLP/OceanGPT-2B-v0.1" target="_blank">2B</a>    |  <a href="https://huggingface.co/zjunlp/OceanGPT-2B-v0.1" target="_blank">2B</a>   |
| OceanGPT-coder-0.6B (åŸºäºQwen3) | <a href="https://www.modelscope.cn/models/ZJUNLP/OceanGPT-coder-0.6B" target="_blank">0.6B</a>    |  <a href="https://huggingface.co/zjunlp/OceanGPT-coder-0.6B" target="_blank">0.6B</a>   |

---

- â—**è¯·æ³¨æ„ï¼Œåœ¨çº¿æ¼”ç¤ºç³»ç»Ÿï¼ˆåŒ…æ‹¬è§†é¢‘ï¼‰ä¸­çš„æµ·æ´‹é¢†åŸŸé—®ç­”åŸºäºçŸ¥è¯†åº“å¢å¼ºå’Œ"é€šä¸“ç»“åˆ"æ–¹æ³•ï¼Œç”Ÿæˆçš„å†…å®¹ä¸å¼€æºæ¨¡å‹å­˜åœ¨å·®å¼‚ï¼**
- â—**ç”±äºè®¡ç®—èµ„æºæœ‰é™ï¼ŒOceanGPT-oç›®å‰ä»…é€‚ç”¨äºæŸäº›ç±»å‹å£°çº³å›¾åƒå’Œæµ·æ´‹ç§‘å­¦å›¾åƒçš„è‡ªç„¶è¯­è¨€è§£é‡Šå’Œç”Ÿæˆã€‚å»ºè®®ä½¿ç”¨å¤§äºæˆ–ç­‰äº24GBçš„GPU**

### æŒ‡ä»¤æ•°æ®

| æ•°æ®åç§°        | HuggingFace                                                                                                                    | ModelScope                                                                |
|-------------------|----------------------------------------------------------------------------------- |-----------------------------------------------------------------------------------------|
| OceanInstruct-v0.2  | <a href="https://huggingface.co/datasets/zjunlp/OceanInstruct-v0.2" target="_blank">50K</a>   | <a href="https://modelscope.cn/datasets/ZJUNLP/OceanInstruct-v0.2" target="_blank">50K</a> |
| OceanInstruct-o  | <a href="https://huggingface.co/datasets/zjunlp/OceanInstruct-o" target="_blank">50K</a>  | <a href="https://modelscope.cn/datasets/ZJUNLP/OceanInstruct-o" target="_blank">50K</a> |
| OceanInstruct-v0.1  | <a href="https://huggingface.co/datasets/zjunlp/OceanInstruct-v0.1" target="_blank">10K</a>  | <a href="https://modelscope.cn/datasets/ZJUNLP/OceanInstruct-v0.1" target="_blank">10K</a> |
---
- â—**éƒ¨åˆ†æŒ‡ä»¤æ•°æ®ä¸ºåˆæˆæ•°æ®ï¼›å¦‚å­˜åœ¨é”™è¯¯æ•¬è¯·è°…è§£ï¼ˆéƒ¨åˆ†æŒ‡ä»¤æ•°æ®ä¸ºåˆæˆæ•°æ®ï¼Œå¦‚å­˜åœ¨é”™è¯¯æ•¬è¯·è°…è§£ï¼‰ï¼**

## ğŸŒŸæ¦‚è¿°

è¿™æ˜¯OceanGPT (æ²§æ¸Š) é¡¹ç›®ï¼Œæ—¨åœ¨ä¸ºæµ·æ´‹ç§‘å­¦ä»»åŠ¡æ„å»ºå¤§è¯­è¨€æ¨¡å‹ã€‚

- â—**å…è´£å£°æ˜ï¼šæœ¬é¡¹ç›®çº¯å±å­¦æœ¯æ¢ç´¢ï¼Œå¹¶éäº§å“åº”ç”¨ï¼ˆæœ¬é¡¹ç›®ä»…ä¸ºå­¦æœ¯æ¢ç´¢å¹¶éäº§å“åº”ç”¨ï¼‰ã€‚è¯·æ³¨æ„ï¼Œç”±äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å›ºæœ‰å±€é™æ€§ï¼Œå¯èƒ½ä¼šå‡ºç°å¹»è§‰ç­‰é—®é¢˜ã€‚**

<div align="center">
<img src="figs/overview.png" width="60%">
<img src="figs/vedio.gif" width="60%">
</div>


## â©å¿«é€Ÿå¼€å§‹

```
conda create -n py3.11 python=3.11
conda activate py3.11
pip install -r requirements.txt
```

### ä¸‹è½½æ¨¡å‹
#### ä»HuggingFaceä¸‹è½½
```shell
git lfs install
git clone https://huggingface.co/zjunlp/OceanGPT-14B-v0.1
```
æˆ–
```
huggingface-cli download --resume-download zjunlp/OceanGPT-14B-v0.1 --local-dir OceanGPT-14B-v0.1 --local-dir-use-symlinks False
```
#### ä»WiseModelä¸‹è½½
```shell
git lfs install
git clone https://www.wisemodel.cn/zjunlp/OceanGPT-14B-v0.1.git
```
#### ä»ModelScopeä¸‹è½½
```shell
git lfs install
git clone https://www.modelscope.cn/ZJUNLP/OceanGPT-14B-v0.1.git
```

### æ¨ç†
#### OceanGPT-basic-8B
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "zjunlp/OceanGPT-basic-8B"

# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

question = "<æ‚¨çš„é—®é¢˜>"
messages = [
    {"role": "user", "content": question}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=False 
)

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=8192
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

try:
    index = len(output_ids) - output_ids[::-1].index(151668)  # </think> token ID
except ValueError:
    index = 0

content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")
print(content)
```

#### OceanGPT-o-7B
```shell
# å¼ºçƒˆå»ºè®®ä½¿ç”¨`[decord]`åŠŸèƒ½ä»¥æ›´å¿«åœ°åŠ è½½è§†é¢‘ã€‚
pip install qwen-vl-utils[decord]==0.0.8
pip install transformers
```
```python
from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, Qwen2VLProcessor
from qwen_vl_utils import process_vision_info
import torch

model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "zjunlp/OceanGPT-o-7B", torch_dtype=torch.bfloat16, device_map="auto"
)
processor = Qwen2VLProcessor.from_pretrained("zjunlp/OceanGPT-o-7B")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "file:///path/to/your/image.jpg",
            },
            {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡ã€‚"},
        ],
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
inputs = inputs.to("cuda")


generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```

#### OceanGPT-coder-7B
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained(
    "zjunlp/OceanGPT-coder-7B", torch_dtype=torch.float16, device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("zjunlp/OceanGPT-coder-7B")
messages = [
    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
    {"role": "user", "content": "è¯·ä¸ºæ°´ä¸‹æœºå™¨äººç”ŸæˆMOOSä»£ç ï¼Œå®ç°å¦‚ä¸‹ä»»åŠ¡ï¼šå…ˆå›åˆ°ï¼ˆ50,20ï¼‰ç‚¹ï¼Œç„¶åä»¥ï¼ˆ15,20ï¼‰ç‚¹ä¸ºåœ†å½¢ï¼ŒåšåŠå¾„ä¸º30çš„åœ†å‘¨è¿åŠ¨ï¼ŒæŒç»­æ—¶é—´200sï¼Œé€Ÿåº¦4 m/sã€‚"}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    top_p=0.6,
    temperature=0.6,
    max_new_tokens=2048
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```
#### ä½¿ç”¨vllmè¿›è¡Œæ¨ç†
```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

path = 'YOUR-MODEL-PATH'

tokenizer = AutoTokenizer.from_pretrained(path)

prompt = "Which is the largest ocean in the world?"
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

sampling_params = SamplingParams(temperature=0.8, top_k=50)
llm = LLM(model=path)

response = llm.generate(text, sampling_params)
```

## ğŸ¤—ä¸æˆ‘ä»¬çš„Gradioæ¼”ç¤ºå¯¹è¯

### æœ¬åœ°WebUIæ¼”ç¤º
æ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›çš„ä»£ç è½»æ¾éƒ¨ç½²æœ¬åœ°äº¤äº’ç•Œé¢ã€‚

> ğŸ”§ è¿è¡Œå‰ï¼Œè¯·ä¿®æ”¹app.pyä¸­çš„æ¨¡å‹è·¯å¾„ï¼ˆOceanGPT/OceanGPT-o/OceanGPT-coderçš„è·¯å¾„ï¼‰ä¸ºæ‚¨æœ¬åœ°çš„æ¨¡å‹è·¯å¾„ã€‚

```shell
python app.py
```
åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€`https://localhost:7860/`å¹¶äº«å—ä¸OceanGPTçš„äº¤äº’ã€‚

### åœ¨çº¿æ¼”ç¤º <!-- omit in toc -->
#### æµ·æ´‹ä¸“ä¸šçŸ¥è¯†é—®ç­”
<table>
    <tr>
        <td><img src="figs/3.png"></td>
        <td><img src="figs/4.png"></td>
    </tr>
</table>
æ‚¨å¯ä»¥ä½¿ç”¨OceanGPT-basicè¿›è¡Œæµ·æ´‹ä¸“ä¸šçŸ¥è¯†é—®ç­”ã€‚

1. è¾“å…¥æ‚¨çš„æŸ¥è¯¢ï¼ˆå¯é€‰ï¼šä¸Šä¼ Word/PDFæ–‡ä»¶ï¼‰ã€‚
2. é€‰æ‹©ç”Ÿæˆè¶…å‚æ•°ã€‚
3. è¿è¡Œå¹¶è·å–ç»“æœã€‚
   
#### æµ·æ´‹ç§‘å­¦å›¾åƒè§£é‡Š
<table>
    <tr>
        <td><img src="figs/1.png"></td>
        <td><img src="figs/2.png"></td>
    </tr>
</table>
æ‚¨å¯ä»¥ä½¿ç”¨OceanGPT-oè¿›è¡Œæµ·æ´‹ç§‘å­¦å›¾åƒè§£é‡Šã€‚

1. è¾“å…¥æ‚¨çš„æŸ¥è¯¢å¹¶ä¸Šä¼ å›¾ç‰‡ã€‚
2. é€‰æ‹©ç”Ÿæˆè¶…å‚æ•°ã€‚
3. è¿è¡Œå¹¶è·å–ç»“æœã€‚

#### æµ·æ´‹å£°çº³å›¾åƒè§£é‡Š
<table>
    <tr>
        <td><img src="figs/1.png"></td>
        <td><img src="figs/7.png"></td>
    </tr>
</table>
æ‚¨å¯ä»¥ä½¿ç”¨OceanGPT-oè¿›è¡Œæµ·æ´‹å£°çº³å›¾åƒè§£é‡Šã€‚

1. è¾“å…¥æ‚¨çš„æŸ¥è¯¢å¹¶ä¸Šä¼ å›¾ç‰‡ã€‚
2. é€‰æ‹©ç”Ÿæˆè¶…å‚æ•°ã€‚
3. è¿è¡Œå¹¶è·å–ç»“æœã€‚



#### æ°´ä¸‹æœºå™¨äººMOOSä»£ç ç”Ÿæˆ
<table>
    <tr>
        <td><img src="figs/5.png"></td>
        <td><img src="figs/6.png"></td>
    </tr>
</table>
æ‚¨å¯ä»¥ä½¿ç”¨OceanGPT-coderè¿›è¡Œmoosä»£ç ç”Ÿæˆã€‚

1. è¾“å…¥æ‚¨çš„æŸ¥è¯¢ã€‚
2. é€‰æ‹©ç”Ÿæˆè¶…å‚æ•°ã€‚
3. è¿è¡Œå¹¶ç”Ÿæˆä»£ç ã€‚

## åŸºäº OceanGPT æ„å»ºå®šåˆ¶åŒ–é—®ç­”åº”ç”¨

æœ¬æ•™ç¨‹åŸºäº OceanGPTÂ·æ²§æ¸Š å¼€æºå¤§æ¨¡å‹ï¼Œç»“åˆ EasyDataset å¼€æºå·¥å…·ä¸ Llama Factory å¼€æºå·¥å…·ï¼Œæ¶µç›–ä»¥ä¸‹å…³é”®æ­¥éª¤ï¼š

* æ¨¡å‹è·å–
* EasyDataset æ•°æ®å·¥ç¨‹å¤„ç†
* ä½¿ç”¨ Llama Factory è¿›è¡Œé¢†åŸŸå¾®è°ƒ
* æ„å»º Web åº”ç”¨
* ç”¨æˆ·ä½¿ç”¨ä¸æ•ˆæœéªŒè¯

æœ¬æŒ‡å—æä¾›äº†ä¸€å¥—å®ç”¨çš„å·¥ç¨‹åŒ–è§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©æ‚¨å¿«é€Ÿæ„å»ºé¢å‘æµ·æ´‹é¢†åŸŸçš„ä¸“ä¸šé—®ç­”ç³»ç»Ÿã€‚æœ‰å…³è¯¦ç»†çš„ç¯å¢ƒé…ç½®è¯´æ˜å’Œä½¿ç”¨ç¤ºä¾‹ï¼Œè¯·å‚è§ [CustomQA_CN.md](https://github.com/zjunlp/OceanGPT/blob/main/CustomQA_CN.md) æˆ– [CustomQA_EN.md](https://github.com/zjunlp/OceanGPT/blob/main/CustomQA_EN.md)ã€‚

## ä½¿ç”¨MCPæœåŠ¡å™¨è¿›è¡Œå£°çº³å›¾åƒæè¿°

[mcp_userver](https://github.com/zjunlp/OceanGPT/tree/main/mcp_server)ç›®å½•åŒ…å«OceanGPTçš„æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æœåŠ¡å™¨ï¼Œç”¨äºå®ç°æŸäº›åŠŸèƒ½ã€‚

è¯¦ç»†çš„è®¾ç½®è¯´æ˜å’Œä½¿ç”¨ç¤ºä¾‹ï¼Œè¯·å‚è§MCPæœåŠ¡å™¨[README](https://github.com/zjunlp/OceanGPT/blob/main/mcp_server/README.md)ã€‚

## ğŸ“Œæ¨ç†

### ä½¿ç”¨sglangã€vLLMã€ollamaã€llama.cppè¿›è¡Œé«˜æ•ˆæ¨ç†

<details>
<summary> sglangç°åœ¨æ­£å¼æ”¯æŒåŸºäºQwen2.5-VLå’ŒQwen2.5çš„æ¨¡å‹ã€‚ç‚¹å‡»æŸ¥çœ‹ã€‚ </summary>

1. å®‰è£…sglangï¼š
```shell
pip install --upgrade pip
pip install uv
uv pip install "sglang[all]>=0.4.6.post4"
```

2. å¯åŠ¨æœåŠ¡å™¨ï¼š
```python
import requests
from openai import OpenAI
from sglang.test.test_utils import is_in_ci

if is_in_ci():
    from patch import launch_server_cmd
else:
    from sglang.utils import launch_server_cmd

from sglang.utils import wait_for_server, print_highlight, terminate_process


server_process, port = launch_server_cmd(
    "python3 -m sglang.launch_server --model-path zjunlp/OceanGPT-o-7B --host 0.0.0.0"
)

wait_for_server(f"http://localhost:{port}")
```

3. ä¸æ¨¡å‹èŠå¤©
```python
import requests

url = f"http://localhost:{port}/v1/chat/completions"

data = {
    "model": "Qwen/Qwen2.5-VL-7B-Instruct",
    "messages": [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "What's in this image?"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true"
                    },
                },
            ],
        }
    ],
    "max_tokens": 300,
}

response = requests.post(url, json=data)
print_highlight(response.text)
```


</details>

<details> 
<summary>llama.cppç°åœ¨æ­£å¼æ”¯æŒåŸºäºQwen2.5-hfè½¬æ¢ä¸ºggufçš„æ¨¡å‹ã€‚ç‚¹å‡»å±•å¼€æŸ¥çœ‹ã€‚</summary>

ä»huggingfaceä¸‹è½½OceanGPT PyTorchæ¨¡å‹åˆ°â€œOceanGPTâ€æ–‡ä»¶å¤¹ã€‚

å…‹éš†llama.cppå¹¶ç¼–è¯‘ï¼š
```shell
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
make llama-cli
```

ç„¶åå°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºggufæ–‡ä»¶ï¼š
```shell
python convert-hf-to-gguf.py OceanGPT --outfile OceanGPT.gguf
```

è¿è¡Œæ¨¡å‹ï¼š
```shell
./llama-cli -m OceanGPT.gguf \
    -co -cnv -p "Your prompt" \
    -fa -ngl 80 -n 512
```
  </details>

<details> 
<summary>ollamaç°åœ¨æ­£å¼æ”¯æŒåŸºäºQwen2.5çš„æ¨¡å‹ã€‚ç‚¹å‡»å±•å¼€æŸ¥çœ‹ã€‚</summary>

åˆ›å»ºä¸€ä¸ªåä¸º`Modelfile`çš„æ–‡ä»¶ï¼š
```shell
FROM ./OceanGPT.gguf
TEMPLATE "[INST] {{ .Prompt }} [/INST]"
```

åœ¨Ollamaä¸­åˆ›å»ºæ¨¡å‹ï¼š
```shell
ollama create example -f Modelfile
```

è¿è¡Œæ¨¡å‹ï¼š
```shell
ollama run example "What is your favourite condiment?"
```
  </details>

<details>
<summary> vLLMç°åœ¨æ­£å¼æ”¯æŒåŸºäºQwen2.5-VLå’ŒQwen2.5çš„æ¨¡å‹ã€‚ç‚¹å‡»å±•å¼€æŸ¥çœ‹ã€‚</summary>

1. ä¸‹è½½ vLLM(>=0.7.3):
```shell
pip install vllm
```

2. è¿è¡Œç¤ºä¾‹:
* [MLLM](https://docs.vllm.ai/en/latest/getting_started/examples/vision_language.html) 
* [LLM](https://docs.vllm.ai/en/latest/getting_started/quickstart.html) 
  </details>


## ğŸŒ»è‡´è°¢

OceanGPT (æ²§æ¸Š) åŸºäºå¼€æºå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒï¼ŒåŒ…æ‹¬[Qwen](https://huggingface.co/Qwen), [MiniCPM](https://huggingface.co/collections/openbmb/minicpm-2b-65d48bf958302b9fd25b698f), [LLaMA](https://huggingface.co/meta-llama)ã€‚

OceanGPTåŸºäºå¼€æºæ•°æ®å’Œå·¥å…·è®­ç»ƒï¼ŒåŒ…æ‹¬[Moos](https://github.com/moos-tutorials)ã€[UATD](https://openi.pcl.ac.cn/OpenOrcinus_orca/URPC2021_sonar_images_dataset)ã€[Forward-looking Sonar Detection Dataset](https://github.com/XingYZhu/Forward-looking-Sonar-Detection-Dataset)ã€[NKSID](https://github.com/Jorwnpay/NK-Sonar-Image-Dataset)ã€[SeabedObjects-KLSG](https://github.com/huoguanying/SeabedObjects-Ship-and-Airplane-dataset)ã€[Marine Debris](https://github.com/mvaldenegro/marine-debris-fls-datasets/tree/master/md_fls_dataset/data/turntable-cropped)ã€‚

æ„Ÿè°¢ä»–ä»¬çš„å·¨å¤§è´¡çŒ®ï¼

## å±€é™æ€§

- æ¨¡å‹å¯èƒ½å­˜åœ¨å¹»è§‰é—®é¢˜ã€‚
- æˆ‘ä»¬æœªå¯¹èº«ä»½ä¿¡æ¯è¿›è¡Œä¼˜åŒ–ï¼Œæ¨¡å‹å¯èƒ½ä¼šç”Ÿæˆç±»ä¼¼äºQwen/MiniCPM/LLaMA/GPTç³»åˆ—æ¨¡å‹çš„èº«ä»½ä¿¡æ¯ã€‚
- æ¨¡å‹è¾“å‡ºå—æç¤ºè¯å½±å“ï¼Œå¯èƒ½å¯¼è‡´å¤šæ¬¡å°è¯•ç»“æœä¸ä¸€è‡´ã€‚
- æ¨¡å‹éœ€è¦åŒ…å«ç‰¹å®šæ¨¡æ‹Ÿå™¨ä»£ç æŒ‡ä»¤è¿›è¡Œè®­ç»ƒæ‰èƒ½å…·å¤‡æ¨¡æ‹Ÿå…·èº«æ™ºèƒ½èƒ½åŠ›ï¼ˆæ¨¡æ‹Ÿå™¨å—ç‰ˆæƒé™åˆ¶ï¼Œæš‚æ— æ³•å…¬å¼€ï¼‰ï¼Œå…¶å½“å‰èƒ½åŠ›éå¸¸æœ‰é™ã€‚

### ğŸš©å¼•ç”¨

å¦‚æœæ‚¨åœ¨å·¥ä½œä¸­ä½¿ç”¨äº†OceanGPTï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ã€‚

```bibtex
@article{bi2024oceangpt,
  title={OceanGPT: A Large Language Model for Ocean Science Tasks},
  author={Bi, Zhen and Zhang, Ningyu and Xue, Yida and Ou, Yixin and Ji, Daxiong and Zheng, Guozhou and Chen, Huajun},
  journal={arXiv preprint arXiv:2310.02031},
  year={2024}
}

```

---
# è´¡çŒ®è€…

[Ningyu Zhang](https://person.zju.edu.cn/en/ningyu)ã€Yida Xueã€Zhen Biã€Xiaozhuan Liangã€Zhisong Qiuã€Kewei Xuã€Chenxi Wangã€Shumin Dengã€Xiangyuan Ruã€Jintian Zhangã€Shuofei Qiaoã€Guozhou Zhengã€Huajun Chen


ç¤¾åŒºè´¡çŒ®è€…ï¼šJunjie Zhengã€Zhe Maã€Shuwei Pengã€Song Gao 