<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>
<br>
<div align="center">
<img src="figs/logo.jpg" width="300px">

**OceanGPT (æ²§æ¸Š): æ²§æ¸Šæµ·æ´‹åŸºç¡€å¤§æ¨¡å‹**

<p align="center">
    <a href="https://github.com/zjunlp/OceanGPT">é¡¹ç›®</a> â€¢
    <a href="https://arxiv.org/abs/2310.02031">è®ºæ–‡</a> â€¢
    <a href="https://huggingface.co/collections/zjunlp/oceangpt-664cc106358fdd9f09aa5157">æ¨¡å‹</a> â€¢
    <a href="http://oceangpt.zjukg.cn/">ç½‘ç«™</a> â€¢
    <a href="#æ¦‚è¿°">æ¦‚è¿°</a> â€¢
    <a href="#å¿«é€Ÿå¼€å§‹">å¿«é€Ÿå¼€å§‹</a> â€¢
    <a href="#å¼•ç”¨">å¼•ç”¨</a>
</p>

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
![](https://img.shields.io/badge/PRs-Welcome-red)  <a href='https://hyper.ai/datasets/32992'><img src='https://img.shields.io/badge/Dataset-HyperAIè¶…ç¥ç»-pink'></a> 


</div>

## Table of Contents

- <a href="#æœ€æ–°åŠ¨æ€">æœ€æ–°åŠ¨æ€</a>
- <a href="#æ¦‚è¿°">æ¦‚è¿°</a>
- <a href="#å¿«é€Ÿå¼€å§‹">å¿«é€Ÿå¼€å§‹</a>
- <a href="#ä¸æˆ‘ä»¬çš„Gradioæ¼”ç¤ºå¯¹è¯"> ğŸ¤—ä¸æˆ‘ä»¬çš„Gradioæ¼”ç¤ºå¯¹è¯</a>
- <a href="#æ¨ç†">æ¨ç†</a>
- <a href="#æ¨¡å‹">æ¨¡å‹</a>
- <a href="#ä½¿ç”¨llama.cpp, ollama, vLLMè¿›è¡Œé«˜æ•ˆæ¨ç†">ä½¿ç”¨llama.cpp, ollama, vLLMè¿›è¡Œé«˜æ•ˆæ¨ç†</a>
- <a href="#å¼•ç”¨">å¼•ç”¨</a>

## ğŸ””æœ€æ–°åŠ¨æ€
- **2024-07-04ï¼Œæˆ‘ä»¬å‘å¸ƒäº†OceanGPT-Basic-14B/2Bä»¥åŠæ›´æ–°ç‰ˆæœ¬çš„OceanGPT-Basic-7Bã€‚**
- **2024-06-04ï¼ŒOceanGPT è¢«ACL 2024æ¥æ”¶ã€‚ğŸ‰ğŸ‰**
- **2023-10-04ï¼Œæˆ‘ä»¬å‘å¸ƒäº†è®ºæ–‡"OceanGPT: A Large Language Model for Ocean Science Tasks "å¹¶åŸºäºLLaMA2å‘å¸ƒäº†OceanGPT-Basic-7Bã€‚**
- **2023-05-01ï¼Œæˆ‘ä»¬å¯åŠ¨äº†OceanGPT (æ²§æ¸Š) é¡¹ç›®ã€‚**
---

## ğŸŒŸæ¦‚è¿°

è¿™æ˜¯OceanGPT (æ²§æ¸Š) é¡¹ç›®ï¼Œæ—¨åœ¨ä¸ºæµ·æ´‹ç§‘å­¦ä»»åŠ¡æ„å»ºå¤§è¯­è¨€æ¨¡å‹ã€‚

- â—**å…è´£å£°æ˜ï¼šæœ¬é¡¹ç›®çº¯å±å­¦æœ¯æ¢ç´¢ï¼Œå¹¶éäº§å“åº”ç”¨ï¼ˆæœ¬é¡¹ç›®ä»…ä¸ºå­¦æœ¯æ¢ç´¢å¹¶éäº§å“åº”ç”¨ï¼‰ã€‚è¯·æ³¨æ„ï¼Œç”±äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å›ºæœ‰å±€é™æ€§ï¼Œå¯èƒ½ä¼šå‡ºç°å¹»è§‰ç­‰é—®é¢˜ã€‚**

<div align="center">
<img src="figs/overview.png" width="60%">
<img src="figs/vedio.gif" width="60%">
</div>


## â©å¿«é€Ÿå¼€å§‹

```
conda create -n py3.11 python=3.11
conda activate py3.11
pip install -r requirements.txt
```

### ä¸‹è½½æ¨¡å‹
#### ä»HuggingFaceä¸‹è½½
```shell
git lfs install
git clone https://huggingface.co/zjunlp/OceanGPT-14B-v0.1
```
æˆ–
```
huggingface-cli download --resume-download zjunlp/OceanGPT-14B-v0.1 --local-dir OceanGPT-14B-v0.1 --local-dir-use-symlinks False
```
#### ä»WiseModelä¸‹è½½
```shell
git lfs install
git clone https://www.wisemodel.cn/zjunlp/OceanGPT-14B-v0.1.git
```
#### ä»ModelScopeä¸‹è½½
```shell
git lfs install
git clone https://www.modelscope.cn/ZJUNLP/OceanGPT-14B-v0.1.git
```

### æ¨ç†
#### ä½¿ç”¨HuggingFaceè¿›è¡Œæ¨ç†
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

device = "cuda"
path = 'YOUR-MODEL-PATH'

model = AutoModelForCausalLM.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(path)

prompt = "Which is the largest ocean in the world?"
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(device)

generated_ids = model.generate(
    model_inputs.input_ids,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```
#### ä½¿ç”¨vllmè¿›è¡Œæ¨ç†
```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

path = 'YOUR-MODEL-PATH'

tokenizer = AutoTokenizer.from_pretrained(path)

prompt = "Which is the largest ocean in the world?"
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

sampling_params = SamplingParams(temperature=0.8, top_k=50)
llm = LLM(model=path)

response = llm.generate(text, sampling_params)
```

## ğŸ¤—ä¸æˆ‘ä»¬çš„Gradioæ¼”ç¤ºå¯¹è¯

### åœ¨çº¿æ¼”ç¤º <!-- omit in toc -->

æˆ‘ä»¬ä¸ºç”¨æˆ·æä¾›äº†å¯é€šè¿‡ç½‘ç»œè®¿é—®çš„äº¤äº’å¼Gradioæ¼”ç¤º

### æœ¬åœ°WebUIæ¼”ç¤º
You can easily deploy the interactive interface locally using the code we provide.

```python
python app.py
```
åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ `https://localhost:7860/` å¹¶äº«å—ä¸OceanGPTçš„äº’åŠ¨ã€‚

## ğŸ“Œæ¨ç†

### æ¨¡å‹

| æ¨¡å‹åç§°        | HuggingFace                                                          | WiseModel                                                                 | ModelScope                                                                |
|-------------------|-----------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|
| OceanGPT-Basic-14B (based on Qwen) | <a href="https://huggingface.co/zjunlp/OceanGPT-14B-v0.1" target="_blank">14B</a> | <a href="https://wisemodel.cn/models/zjunlp/OceanGPT-14B-v0.1" target="_blank">14B</a> | <a href="https://modelscope.cn/models/ZJUNLP/OceanGPT-14B-v0.1" target="_blank">14B</a> |
| OceanGPT-Basic-7B (based on Qwen) | <a href="https://huggingface.co/zjunlp/OceanGPT-7b-v0.2" target="_blank">7B</a>   | <a href="https://wisemodel.cn/models/zjunlp/OceanGPT-7b-v0.2" target="_blank">7B</a>   | <a href="https://modelscope.cn/models/ZJUNLP/OceanGPT-7b-v0.2" target="_blank">7B</a>   |
| OceanGPT-Basic-2B (based on MiniCPM) | <a href="https://huggingface.co/zjunlp/OceanGPT-2B-v0.1" target="_blank">2B</a>   | <a href="https://wisemodel.cn/models/zjunlp/OceanGPT-2b-v0.1" target="_blank">2B</a>   | <a href="https://modelscope.cn/models/ZJUNLP/OceanGPT-2B-v0.1" target="_blank">2B</a>   |
| OceanGPT-Omni-7B  | å³å°†å‘å¸ƒ                                                                    | å³å°†å‘å¸ƒ                                                                         | å³å°†å‘å¸ƒ                                                                          |
| OceanGPT-Coder-7B  | å³å°†å‘å¸ƒ                                                                    | å³å°†å‘å¸ƒ                                                                         | å³å°†å‘å¸ƒ                                                                          |
---

### ä½¿ç”¨llama.cppã€ollamaã€vLLMè¿›è¡Œé«˜æ•ˆæ¨ç†

<details> 
<summary>llama.cppç°åœ¨æ­£å¼æ”¯æŒåŸºäºQwen2.5-hfè½¬æ¢ä¸ºggufçš„æ¨¡å‹ã€‚ç‚¹å‡»å±•å¼€æŸ¥çœ‹ã€‚</summary>

ä»huggingfaceä¸‹è½½OceanGPT PyTorchæ¨¡å‹åˆ°â€œOceanGPTâ€æ–‡ä»¶å¤¹ã€‚

å…‹éš†llama.cppå¹¶ç¼–è¯‘ï¼š
```shell
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
make llama-cli
```

ç„¶åå°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºggufæ–‡ä»¶ï¼š
```shell
python convert-hf-to-gguf.py OceanGPT --outfile OceanGPT.gguf
```

è¿è¡Œæ¨¡å‹ï¼š
```shell
./llama-cli -m OceanGPT.gguf \
    -co -cnv -p "Your prompt" \
    -fa -ngl 80 -n 512
```
  </details>

<details> 
<summary>ollamaç°åœ¨æ­£å¼æ”¯æŒåŸºäºQwen2.5çš„æ¨¡å‹ã€‚ç‚¹å‡»å±•å¼€æŸ¥çœ‹ã€‚</summary>

åˆ›å»ºä¸€ä¸ªåä¸º`Modelfile`çš„æ–‡ä»¶ï¼š
```shell
FROM ./OceanGPT.gguf
TEMPLATE "[INST] {{ .Prompt }} [/INST]"
```

åœ¨Ollamaä¸­åˆ›å»ºæ¨¡å‹ï¼š
```shell
ollama create example -f Modelfile
```

è¿è¡Œæ¨¡å‹ï¼š
```shell
ollama run example "What is your favourite condiment?"
```
  </details>

<details>
<summary> vLLMç°åœ¨æ­£å¼æ”¯æŒåŸºäºQwen2.5-VLå’ŒQwen2.5çš„æ¨¡å‹ã€‚ç‚¹å‡»å±•å¼€æŸ¥çœ‹ã€‚</summary>

1. ä¸‹è½½ vLLM(>=0.7.3):
```shell
pip install vllm
```

2. è¿è¡Œç¤ºä¾‹:
* [MLLM](https://docs.vllm.ai/en/latest/getting_started/examples/vision_language.html) 
* [LLM](https://docs.vllm.ai/en/latest/getting_started/quickstart.html) 
  </details>


## ğŸŒ»è‡´è°¢

OceanGPT (æ²§æ¸Š) åŸºäºå¼€æºå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒï¼ŒåŒ…æ‹¬[Qwen](https://huggingface.co/Qwen), [MiniCPM](https://huggingface.co/collections/openbmb/minicpm-2b-65d48bf958302b9fd25b698f), [LLaMA](https://huggingface.co/meta-llama)ã€‚æ„Ÿè°¢ä»–ä»¬çš„æ°å‡ºè´¡çŒ®ï¼

## å±€é™æ€§

- æ¨¡å‹å¯èƒ½å­˜åœ¨å¹»è§‰é—®é¢˜ã€‚
- æˆ‘ä»¬æœªå¯¹èº«ä»½ä¿¡æ¯è¿›è¡Œä¼˜åŒ–ï¼Œæ¨¡å‹å¯èƒ½ä¼šç”Ÿæˆç±»ä¼¼äºQwen/MiniCPM/LLaMA/GPTç³»åˆ—æ¨¡å‹çš„èº«ä»½ä¿¡æ¯ã€‚
- æ¨¡å‹è¾“å‡ºå—æç¤ºè¯å½±å“ï¼Œå¯èƒ½å¯¼è‡´å¤šæ¬¡å°è¯•ç»“æœä¸ä¸€è‡´ã€‚
- æ¨¡å‹éœ€è¦åŒ…å«ç‰¹å®šæ¨¡æ‹Ÿå™¨ä»£ç æŒ‡ä»¤è¿›è¡Œè®­ç»ƒæ‰èƒ½å…·å¤‡æ¨¡æ‹Ÿå…·èº«æ™ºèƒ½èƒ½åŠ›ï¼ˆæ¨¡æ‹Ÿå™¨å—ç‰ˆæƒé™åˆ¶ï¼Œæš‚æ— æ³•å…¬å¼€ï¼‰ï¼Œå…¶å½“å‰èƒ½åŠ›éå¸¸æœ‰é™ã€‚

### ğŸš©å¼•ç”¨

å¦‚æœæ‚¨åœ¨å·¥ä½œä¸­ä½¿ç”¨äº†OceanGPTï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ã€‚

```bibtex
@article{bi2024oceangpt,
  title={OceanGPT: A Large Language Model for Ocean Science Tasks},
  author={Bi, Zhen and Zhang, Ningyu and Xue, Yida and Ou, Yixin and Ji, Daxiong and Zheng, Guozhou and Chen, Huajun},
  journal={arXiv preprint arXiv:2310.02031},
  year={2024}
}

```

---
